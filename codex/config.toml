model = "gpt-5-codex"
model_reasoning_effort = "high"
hide_agent_reasoning = false

# https://github.com/openai/codex/blob/main/docs/config.md#approval_policy
approval_policy = "on-request"

network_access = true
notify = ["bash", "-lc", "afplay /System/Library/Sounds/Ping.aiff"]

[tools]
web_search = true

[mcp_servers.context7]
command = "bunx"
args = ["-y", "@upstash/context7-mcp@latest"]

[mcp_servers.chrome-devtools]
command = "bunx"
args = ["-y", "chrome-devtools-mcp@latest"]

[mcp_servers.serena]
command = "uvx"
args = [
    "--from",
    "git+https://github.com/oraios/serena",
    "serena",
    "start-mcp-server",
    "--context",
    "codex",
    "--enable-web-dashboard=false",
]

# [mcp_servers.figma]
# startup_timeout_ms = 5_000
# command = "bunx"
# args = ["-y", "mcp-remote", "http://127.0.0.1:3845/sse"]

# ------------------------------------------------------------
# for cloud platform
# ------------------------------------------------------------

# Terraform
[mcp_servers.terraform]
# CAUTION: Before running `codex` commands, you'd better run `docker run -i --rm hashicorp/terraform-mcp-server` to install the server on your local machine.
command = "docker"
args = ["run", "-i", "--rm", "hashicorp/terraform-mcp-server"]
startup_timeout_ms = 10_000

# AWS Terraform MCP server
# CAUTION: Before running `codex` commands, you'd better run `uvx awslabs.terraform-mcp-server@latest` to install the server on your local machine.
[mcp_servers.awslabs-terraform-mcp-server]
command = "uvx"
args = ["awslabs.terraform-mcp-server@latest"]
env = { "FASTMCP_LOG_LEVEL" = "ERROR" }

# AWS Documentation MCP server
# CAUTION: Before running `codex` commands, you'd better run `uvx awslabs.aws-documentation-mcp-server@latest` to install the server on your local machine.
[mcp_servers.awslabs-aws-documentation-mcp-server]
command = "uvx"
args = ["awslabs.aws-documentation-mcp-server@latest"]
env = { "FASTMCP_LOG_LEVEL" = "ERROR", "AWS_DOCUMENTATION_PARTITION" = "aws" }
